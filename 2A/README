NAME: CHEN CHEN
EMAIL: chenchenstudent@gmail.com
ID: 004710308

Files: 
lab2a_add.c:
Source code which implements add with different locks and can run with multipule threads.

lab2a_list.c:
Source code which implements different function with sorted list with different locks and can run with multipule threads.

SortedList.c:
Source code implemented according to SortedList.h

SortedList.h:
header file for sorted list
Makefile:
    options:
        -build (default target) compile all programs (with the -Wall and -Wextra options).
        -tests run all (over 200) specified test cases to generate results in CSV files. Note that the lab2_list program is expected to fail when running multiple threads without synchronization. Make sure that your Makefile continues executing despite such failures (e.g. put a '-' in front of commands that are expected to fail).
        -graphs use gnuplot(1) and the supplied data reduction scripts to generate the required graphs
        -dist create the deliverable tarball
        -clean delete all programs and output created by the Makefile

*.gp: provided code to generate plots
*.csv: data generated by running lab2a_add and lab2_list according to the spec requirment
*.png: the generated plots

#############

QUESTION 2.1.1 - causing conflicts:
Why does it take many iterations before errors are seen?
    Because if iterations become larger, the opportunities for different thread to enter add() increase, the possibility of multiple threads racing in add() increase.

Why does a significantly smaller number of iterations so seldom fail?
    Because if iterations become smaller, the opportunities for different thread to enter add() decrease, the possibility of racing condition decrease.

#############

QUESTION 2.1.2 - cost of yielding:
Why are the --yield runs so much slower?
    The program with multiple thread with option yield, cost extra context switches. the number of context swtiches without yield option context swtiches is controled by OS. With yield is context switches controled by OS + the context switches by the threads.

Where is the additional time going?
    The PCB might be stored on a per-process stack in kernel memory (as opposed to the user-mode call stack), or there may be some specific operating system defined data structure for this information. A handle to the PCB is added to a queue of processes that are ready to run, often called the ready queue.[1]

Is it possible to get valid per-operation timings if we are using the --yield option? If so, explain how. If not, explain why not.
    If OS or code can accurately measure each call to sched_yield(), we can subtract and get per-operation timings.
    It is almost impossible, because it hard to predict want going to happen next or future in computer(ie. it hard to predict the outcomes of the scheduler. In TCP it is hard to predict which package come first).
    Probably in the future, OS with machine learning or some protocol can do so.(eg utp)

#############

QUESTION 2.1.3 - measurement errors:
Why does the average cost per operation drop with increasing iterations?
    because overhead of thread creation becomes less of an effect in the total cost, the average cost per iteration drops with increasing iterations.

If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the "correct" cost is)?
If the program run with a large number of iterations, we can know the correct cost of each iterations. It is the Law of large numbers.

#############

QUESTION 2.1.4 - costs of serialization:
Why do all of the options perform similarly for low numbers of threads?
Because low number of threads mean the possibility of different thread enter critical section, the possibility of using lock decrease. As a result, the performances are similar.

Why do the three protected operations slow down as the number of threads rises?
Because possibility of different thread enter critical section increase, the possibility of using lock increase. As a result, the performances become poor even with more threads.

#############

QUESTION 2.2.1 - scalability of Mutex
The variation in time per mutex-protected operation vs the number of threads in Part-1 (adds) and Part-2 (sorted lists) is linear.

For the adds, it increase then become flat then increase.
The reason of the flat is in each operation of locking and unlocking of the mutex, the increased content for the mutex.

For the lists, the mutex cost per operation is roughly linear with a constant slope.
Because list operations are more expensive than add operations, the importance of overhead of mutex lock for each iterations increase.


Q 2.2.2

For a large number of threads, spin locks have much worse scalability than mutexes. For both add and list, after 2 threads the cost per op for spin locks increases much more shaply than it does for mutexes. The rate of increase for both spin locks and mutexes are roughly linear, but spin locks have a much larger slope. This is because using spin locks is much more expensive than using a mutex, since the spin lock wastes a lot of CPU time spinning when it doesn't have the lock. Since (especially for a large number of threads) we are in a high-contention environment, many threads could simultaneously be wasting a lot of cycles just spinning waiting for a resoruce. This doesn't happen with mutexes. If we only had to hold the lock for an extremely short interval and we knew that contention would be very rare, then a spin lock may be a choice, but it is not scalable in high-contention environments.

[1]https://en.wikipedia.org/wiki/Context_switch#Steps
